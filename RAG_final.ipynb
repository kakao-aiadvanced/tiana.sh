{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137daa7a-c161-4687-bfee-ecc3b11b41a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a60551fa-22bb-4fd9-88f8-62efd66dd83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(url): \n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(\n",
    "                class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    return docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74343c34-d5f2-484b-95cc-a9a4fa81abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever():\n",
    "    urls = [\n",
    "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "        \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "        \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    ]\n",
    "    \n",
    "    docs = []\n",
    "    \n",
    "    for url in urls:\n",
    "        docs.append(load(url))\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5b0b724-6232-493f-a7a4-2365c56d6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_checker(question, retriever):\n",
    "    retriver_result = retriever.invoke(question)\n",
    "    query = \"question: \" + question + \"\\n\\n\"\n",
    "    query += \"content: \" + str(retriver_result)\n",
    "\n",
    "    parser = JsonOutputParser()\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "유저의 question과 content가 들어오면, question과 content가 서로 관련이 있는지 알려줘.\n",
    "관련이 있다면, yes\n",
    "관련이 없다면, no\n",
    "답변해줘\n",
    "\n",
    "Answer the user query.\\n{format_instructions}\\n{query}\\n\n",
    "key는 related로 해줘\n",
    "    \"\"\",\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    result = chain.invoke({\"query\": query})\n",
    "    return result, str(retriver_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1836480f-65f2-4d9a-a550-a4b86b053247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, docs):\n",
    "    query = \"question: \" + question + \"\\n\\n\"\n",
    "    query += \"content: \" + docs\n",
    "    \n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"content를 참고하여 question의 답변을 작성해줘\"\n",
    "            }, \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40afaf5c-bb2e-4b39-b20d-b593be9c95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hallucination_answer(answer, docs):\n",
    "    parser = JsonOutputParser()\n",
    "    \n",
    "    query = \"answer: \" + answer + \"\\n\\n\"\n",
    "    query += \"docs: \" + docs\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "유저의 answer과 docs가 들어오면, docs에 근거하여 답변이 잘 작성되었는지 확인해줘.\n",
    "docs에 없는 내용을 지어냈는지 확인해야해.\n",
    "말을 지어냈다면,  yes\n",
    "docs에 근거했다면, no\n",
    "답변해줘\n",
    "\n",
    "Answer the user query.\\n{format_instructions}\\n{query}\\n\n",
    "key 값은 hallucination으로 해줘\n",
    "    \"\"\",\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    result = chain.invoke({\"query\": query})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f113ba0-a0d7-4ee6-8ae2-7f29a04947a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final(answer, docs):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"content에서 source를 뽑아서, [answer 내용]\\n source: [source]형태로 적어줘\"\n",
    "            }, \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": 'answer: ' + answer + '\\ndocs:' + docs\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b993c82b-ca5c-4117-9898-1417b75915f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(question):\n",
    "    retriever = create_retriever()\n",
    "    print(\"## relevance check ##\")\n",
    "    result, docs = relevance_checker(question, retriever)\n",
    "    print(result)\n",
    "    print()\n",
    "\n",
    "    if result['related'] == 'yes':\n",
    "        for i in range(3):\n",
    "            print(\"## generate answer ##\")\n",
    "            answer = generate_answer(question, docs)\n",
    "            print(answer)\n",
    "            print()\n",
    "    \n",
    "            print(\"## check hallucination ##\")\n",
    "            result = hallucination_answer(answer, docs)\n",
    "            print(result)\n",
    "            print()\n",
    "\n",
    "            if result['hallucination'] == 'no':\n",
    "                print(\"## Fianl ##\")\n",
    "                answer_fin = final(answer, docs)\n",
    "                return answer_fin\n",
    "    else:\n",
    "        print(\"검색 결과가 없습니다.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad22d1ca-686b-4134-a70d-60e5885324ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## relevance check ##\n",
      "{'related': 'yes'}\n",
      "\n",
      "## generate answer ##\n",
      "Agent memory can be categorized into short-term and long-term memory components. \n",
      "\n",
      "1. **Short-Term Memory**: This involves in-context learning, which allows the model to retain information temporarily while processing tasks. It is utilized during immediate interactions within a specific context.\n",
      "\n",
      "2. **Long-Term Memory**: This type of memory provides the capability to retain and recall extensive information over extended periods, often through an external database or vector store that allows for efficient retrieval of information when needed. \n",
      "\n",
      "In addition to these types, there are mechanisms such as the memory stream, which tracks observations and events from the agent's experiences in natural language. This stream can inform the agent's behavior based on relevance, recency, and importance when recalling past experiences. Another key aspect is the reflection mechanism, which enables agents to synthesize memories into higher-level inferences, guiding future behavior based on learned experiences. \n",
      "\n",
      "Overall, the structure of agent memory is critical for enabling autonomous agents to learn, adapt, and function effectively in a variety of tasks.\n",
      "\n",
      "## check hallucination ##\n",
      "{'hallucination': 'no'}\n",
      "\n",
      "## Fianl ##\n",
      "Agent memory can be categorized into short-term and long-term memory components.\n",
      "\n",
      "1. **Short-Term Memory**: This involves in-context learning, which allows the model to retain information temporarily while processing tasks. It is utilized during immediate interactions within a specific context.\n",
      "\n",
      "2. **Long-Term Memory**: This type of memory provides the capability to retain and recall extensive information over extended periods, often through an external database or vector store that allows for efficient retrieval of information when needed.\n",
      "\n",
      "In addition to these types, there are mechanisms such as the memory stream, which tracks observations and events from the agent's experiences in natural language. This stream can inform the agent's behavior based on relevance, recency, and importance when recalling past experiences. Another key aspect is the reflection mechanism, which enables agents to synthesize memories into higher-level inferences, guiding future behavior based on learned experiences.\n",
      "\n",
      "Overall, the structure of agent memory is critical for enabling autonomous agents to learn, adapt, and function effectively in a variety of tasks.\n",
      "source: https://lilianweng.github.io/posts/2023-06-23-agent/\n"
     ]
    }
   ],
   "source": [
    "main(\"agent memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2913399-4e38-4fdd-a3f8-b31e5bfc2f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## relevance check ##\n",
      "{'related': 'no'}\n",
      "\n",
      "검색 결과가 없습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(\"I like apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c200cc1a-af4c-4d56-9888-23e345ed1c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## relevance check ##\n",
      "{'related': 'yes'}\n",
      "\n",
      "## generate answer ##\n",
      "CoT, or Chain-of-Thought, is a prompting technique used in artificial intelligence, particularly in large language models. Introduced by Wei et al. in 2022, CoT prompting involves generating a series of short sentences that outline reasoning logics step by step, known as reasoning chains or rationales. This method is particularly beneficial for solving complex reasoning tasks, especially when using large models with over 50 billion parameters. In contrast, simpler tasks see only marginal improvements from CoT prompting. There are two main types of CoT prompts, one of which includes few-shot CoT, where models are prompted with a few examples of high-quality reasoning chains, whether manually crafted or generated by the model itself.\n",
      "\n",
      "## check hallucination ##\n",
      "{'hallucination': 'no'}\n",
      "\n",
      "## Fianl ##\n",
      "CoT, or Chain-of-Thought, is a prompting technique used in artificial intelligence, particularly in large language models. Introduced by Wei et al. in 2022, CoT prompting involves generating a series of short sentences that outline reasoning logics step by step, known as reasoning chains or rationales. This method is particularly beneficial for solving complex reasoning tasks, especially when using large models with over 50 billion parameters. In contrast, simpler tasks see only marginal improvements from CoT prompting. There are two main types of CoT prompts, one of which includes few-shot CoT, where models are prompted with a few examples of high-quality reasoning chains, whether manually crafted or generated by the model itself.\n",
      " source: [https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/]\n"
     ]
    }
   ],
   "source": [
    "main(\"What is CoT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b58246-b79e-4426-97fb-d9dea435943d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
