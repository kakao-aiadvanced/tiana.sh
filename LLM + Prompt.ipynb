{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea162db8-9738-4105-be3d-100b34541934",
   "metadata": {},
   "source": [
    "# ollama Ïã§Ïäµ\n",
    "* Îã§Ïö¥Î°úÎìú ÎßÅÌÅ¨: https://github.com/ollama/ollama\n",
    "* ÏÇ¨Ïö©Ìïú Î™®Îç∏: llama3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42725411-a17e-4881-a583-d64c048be0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541a46fc-f279-4221-8b7f-5c7a884e5e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just an AI, so I don't have feelings like humans do. But I'm here to help with any questions or topics you'd like to discuss! How about you, though - how's your day going?\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model=\"llama3\", messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fa6602-0697-44f3-afce-836d9a2ce0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòä ÏïàÎÖïÌïòÏÑ∏Ïöî! (Annyeonghaseyo!) How can I help you today? üòä\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model=\"llama3\", messages=[{\"role\": \"user\", \"content\": \"ÏïàÎÖï\"}])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0965b353-7bfb-4972-b469-e76e5a2a652d",
   "metadata": {},
   "source": [
    "Ïã§ÌñâÌï¥Î≥∏ Í∞ÄÏû• ÌÅ∞ Î™®Îç∏ Ïù¥Î¶Ñ: llama3 8b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b5749-fe47-4f61-8e30-c3e69d5a9765",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e41ae-7053-42af-a7d3-a7b8bad39229",
   "metadata": {},
   "source": [
    "ÏÑ†Ìñâ ÏûëÏóÖ\n",
    "* ÌÇ§ ÏÑ§Ï†ï\n",
    "```\n",
    "!export OPENAI_API_KEY=\"your_api_key_here\"\n",
    "```\n",
    "* Ìå®ÌÇ§ÏßÄ Îã§Ïö¥\n",
    "```\n",
    "!pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b2dba1e-1be4-49f2-93e3-1633c7a4826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Code calls itself back,  \\nEach layer unveils the path,  \\nEndless loops of thought.', role='assistant', function_call=None, tool_calls=None, refusal=None, annotations=[])\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a haiku about recursion in programming.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a478b2-7821-4004-9479-b84afe555217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏïàÎÖïÌïòÏÑ∏Ïöî! Ïñ¥ÎñªÍ≤å ÎèÑÏôÄÎìúÎ¶¥ÍπåÏöî?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"ÏïàÎÖï!\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Î™®Îç∏Ïùò ÏùëÎãµ Ï∂úÎ†•\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc6432-c339-4185-ac58-0cfc8f53df76",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "Self attention Íµ¨Ï°∞Î•º Í≥†Î†§Ìïú prompting Î•º ÏûëÏÑ±ÌïòÍ∏∞\n",
    "\n",
    "ÏòàÏãú) best place Î≥¥Îã§ tourist atrraction Ïù¥ ÏßàÎ¨∏Ïùò ÏùòÎèÑÏóê Îçî Î™ÖÌôïÌïòÍ≤å Î∂ÄÌï©ÌïòÎäî embedding Í∞íÏùÑ Ï†úÍ≥µÌï† Í≤É\n",
    "```\n",
    "What are the best places in Korea?\n",
    "> What are the tourist attractions in Korea?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88776cb-2e6b-4fa3-bbbd-5cce24432751",
   "metadata": {},
   "source": [
    "### Few shot engineering\n",
    "\n",
    "1. ÏòÅÏñ¥Î•º ÌïúÍµ≠Ïñ¥Î°ú Î≤àÏó≠ÌïòÎäî 5-shot prompt Î•º ÏûëÏÑ±ÌïòÍ≥† \"dog\"Î•º Î≤àÏó≠Ìï¥Î≥¥ÏÑ∏Ïöî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8a77ea-5fcd-4fe7-93c5-039cc3f9c6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Í∞ïÏïÑÏßÄ\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"ÏòÅÏñ¥Í∞Ä Îì§Ïñ¥Ïò§Î©¥ ÌïúÍ∏ÄÎ°ú Î≤àÏó≠Ìï¥Ï§ò \\n ex) {input: cat, output: Í≥†ÏñëÏù¥}, {input: book, output: Ï±Ö}, {input: computer output: Ïª¥Ìì®ÌÑ∞}, {input: coffee, output: Ïª§Ìîº}, {input: water, output: Î¨º}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"dog\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Î™®Îç∏Ïùò ÏùëÎãµ Ï∂úÎ†•\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ba9cc-aa0d-4683-9205-0092d50eb27a",
   "metadata": {},
   "source": [
    "2. ÏòÅÌôî Î¶¨Î∑∞Ïóê ÎåÄÌïú sentiment (positive or negative) Î•º Í≤∞Ï†ïÌïòÎäî 5-shot prompt Î•º ÏûëÏÑ±ÌïòÍ≥† \"The storyline was dull and uninspiring.\" Ïóê ÎåÄÌïú Í≤∞Í≥ºÎ•º ÌôïÏù∏Ìï¥Î≥¥ÏÑ∏Ïöî."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9cba2eb-f9ea-4f6b-9747-c3f4df6ceb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{input: The storyline was dull and uninspiring, output: negative}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Î¶¨Î∑∞Í∞Ä ÏûÖÎ†•ÎêòÎ©¥ positive, negativeÎ°ú Î∂ÑÎ•òÌï¥Ï§ò \\n ex) {input: Ïû¨ÎØ∏ÏóÜÎã§, output: negative}, {input: ÏãúÍ∞Ñ Í∞ÄÎäî Ï§Ñ Î™∞ÎûêÏñ¥Ïöî, output: positive}, {input: Îã§ÏãúÎäî Ïïà Î≥º ÎìØ output: negative}, {input: ÎØøÍ≥† Î≥¥Îäî Í∞êÎèÖ ÏûëÌíà, output: positive}, {input: Îèà ÏïÑÍπùÎã§, output: negative}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The storyline was dull and uninspiring\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Î™®Îç∏Ïùò ÏùëÎãµ Ï∂úÎ†•\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb3383-ea5c-4b7f-a013-58a210c42ac8",
   "metadata": {},
   "source": [
    "3. ÏûêÏó∞Ïñ¥Î•º SQL ÏøºÎ¶¨Î°ú Î∞îÍøîÏ£ºÎäî few-shot prompt Î•º ÏûëÏÑ±Ìï¥Î≥¥ÏÑ∏Ïöî."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09eff820-2ef1-4301-bc51-4635ac6ad66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the SQL query to find the average salary of employees in the marketing department:\n",
      "\n",
      "```sql\n",
      "SELECT AVG(salary) FROM employees WHERE department = 'Marketing';\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": '''\n",
    "Convert the following natural language requests into SQL queries:\n",
    "1. \"Í∏âÏó¨Í∞Ä 50,000 Ïù¥ÏÉÅÏù∏ Î™®Îì† ÏßÅÏõê Ï†ïÎ≥¥Î•º Î≥¥Ïó¨Ï£ºÏÑ∏Ïöî.\": SELECT * FROM employees WHERE salary > 50000;\n",
    "2. \"Ïû¨Í≥†Í∞Ä 0Ïù∏ Î™®Îì† Ï†úÌíà Ï†ïÎ≥¥Î•º Î≥¥Ïó¨Ï£ºÏÑ∏Ïöî.\": SELECT * FROM products WHERE stock = 0;\n",
    "3. \"ÏàòÌïô Ï†êÏàòÍ∞Ä 90Ï†ê Ïù¥ÏÉÅÏù∏ ÌïôÏÉùÎì§Ïùò Ïù¥Î¶ÑÏùÑ ÎÇòÏó¥Ìï¥Ï£ºÏÑ∏Ïöî.\": SELECT name FROM students WHERE math_score > 90;\n",
    "4. \"ÏµúÍ∑º 30Ïùº Ïù¥ÎÇ¥Ïóê Ï£ºÎ¨∏Îêú Î™®Îì† Ï£ºÎ¨∏ Ï†ïÎ≥¥Î•º Î≥¥Ïó¨Ï£ºÏÑ∏Ïöî.\": SELECT * FROM orders WHERE order_date >= DATE_SUB(CURDATE(), INTERVAL 30 DAY);\n",
    "5. \"ÎèÑÏãúÎ≥Ñ Í≥†Í∞ù ÏàòÎ•º Î≥¥Ïó¨Ï£ºÏÑ∏Ïöî.\": SELECT city, COUNT(*) FROM customers GROUP BY city;\n",
    "            '''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Find the average salary of employees in the marketing department.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Î™®Îç∏Ïùò ÏùëÎãµ Ï∂úÎ†•\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390af990-c0aa-4436-b84e-38e31ff0af09",
   "metadata": {},
   "source": [
    "### Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43a7ed23-531d-4b31-b81d-b4e6bc78c1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's solve the problem step-by-step: 345 + 678 - 123.\n",
      "\n",
      "First, we will perform the addition (345 + 678).\n",
      "\n",
      "### Step 1: Addition\n",
      "1. ÏùºÏùò ÏûêÎ¶¨ Ïà´ÏûêÎÅºÎ¶¨ ÎçîÌïúÎã§: 5 + 8 = 13 (ÏùºÏùò ÏûêÎ¶¨ ÏàòÎäî 3, 1ÏùÄ Ïò¨Î¶º)\n",
      "2. Ïã≠Ïùò ÏûêÎ¶¨ Ïà´ÏûêÎÅºÎ¶¨ ÎçîÌïúÎã§: 4 + 7 + 1 (Ïò¨Î¶º) = 12 (Ïã≠Ïùò ÏûêÎ¶¨ ÏàòÎäî 2, 1ÏùÄ Ïò¨Î¶º)\n",
      "3. Î∞±Ïùò ÏûêÎ¶¨ Ïà´ÏûêÎÅºÎ¶¨ ÎçîÌïúÎã§: 3 + 6 + 1 (Ïò¨Î¶º) = 10 (Î∞±Ïùò ÏûêÎ¶¨ ÏàòÎäî 0, 1ÏùÄ Ïò¨Î¶º)\n",
      "\n",
      "Í≤∞Í≥º: 1023\n",
      "\n",
      "### Step 2: Subtraction\n",
      "Now, we need to subtract 123 from 1023.\n",
      "\n",
      "1. ÏùºÏùò ÏûêÎ¶¨ Ïà´ÏûêÎÅºÎ¶¨ Î∫ÄÎã§: 3 - 3 = 0\n",
      "2. Ïã≠Ïùò ÏûêÎ¶¨ Ïà´ÏûêÎÅºÎ¶¨ Î∫ÄÎã§: 2 - 2 = 0\n",
      "3. Î∞±Ïùò ÏûêÎ¶¨ Ïà´ÏûêÎÅºÎ¶¨ Î∫ÄÎã§: 10 - 1 = 9 (Ïó¨Í∏∞ÏÑú 10ÏùÄ Ïò¨Î¶ºÏùÑ Í≥†Î†§Ìïú Í≤∞Í≥ºÏûÖÎãàÎã§)\n",
      "\n",
      "Í≤∞Í≥º: 900\n",
      "\n",
      "### Final Answer:\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "# Simple\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": '''\n",
    "# Simple - 1\n",
    "Solve the following problem step-by-step: 23 + 47\n",
    "\n",
    "Step-by-step solution:\n",
    "1. ÏùºÏùò ÏûêÎ¶¨ Ïà´ÏûêÎÅºÎ¶¨ ÎçîÌïúÎã§. 3 + 7 = 10\n",
    "2. Ïã≠Ïùò ÏûêÎ¶¨ Ïà´ÏûêÎÅºÎ¶¨ ÎçîÌïúÎã§. 20 + 40 = 60\n",
    "3. 1Î≤àÍ≥º 2Î≤àÏùò Í≤∞Í≥ºÎ•º ÎçîÌïúÎã§. 10 + 60 = 70\n",
    "\n",
    "Answer: 70\n",
    "\n",
    "# Simple - 2\n",
    "Solve the following problem step-by-step: 123 - 58\n",
    "\n",
    "Step-by-step solution:\n",
    "1. Î∞±Ïùò ÏûêÎ¶¨ Ïà´ÏûêÎÅºÎ¶¨ Î∫ÄÎã§. 1 - 0 = 1\n",
    "2. Ïã≠Ïùò ÏûêÎ¶¨ Ïà´ÏûêÎÅºÎ¶¨ Î∫ÄÎã§. 2 - 5 (x)\n",
    "3. 2Í∞Ä 5Î≥¥Îã§ ÏûëÏïÑ Î∫ÑÏÖàÏù¥ Î∂àÍ∞ÄÎä•ÌïòÎØÄÎ°ú Î∞±Ïùò ÏûêÎ¶¨ Ïà´Ïûê 1ÏùÑ Í∞ÄÏ†∏Ïò®Îã§. 12 - 5 = 7 ÎòêÌïú Î∞±Ïùò ÏûêÎ¶¨ÏóêÏÑú 1ÏùÑ Î∫ÄÎã§. 1 - 1 = 0\n",
    "4. ÏùºÏùò ÏûêÎ¶¨ Ïà´ÏûêÎÅºÎ¶¨ Î∫ÄÎã§. 3 - 8 (x)\n",
    "5. 3Ïù¥ 8Î≥¥Îã§ ÏûëÏïÑ Î∫ÑÏÖàÏù¥ Î∂àÍ∞ÄÎä•ÌïòÎØÄÎ°ú Ïã≠Ïùò ÏûêÎ¶¨ÏóêÏÑú 1ÏùÑ Í∞ÄÏ†∏Ïò®Îã§. 13 - 8 = 5 ÎòêÌïú Ïã≠Ïùò ÏûêÎ¶¨ÏóêÏÑú 1ÏùÑ Î∫ÄÎã§. 7 - 1 = 6\n",
    "\n",
    "Answer: 65\n",
    "            '''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Solve the following problem step-by-step: 345 + 678 - 123\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Î™®Îç∏Ïùò ÏùëÎãµ Ï∂úÎ†•\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "806f75f6-eaf7-4e17-89c4-23c23f667c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this logic puzzle step-by-step, let's analyze the clues provided:\n",
      "\n",
      "1. A is not next to B.\n",
      "2. B is next to C.\n",
      "3. C is not next to D.\n",
      "\n",
      "Let's denote the four positions in the row as 1, 2, 3, and 4. We'll look for possible arrangements based on the clues.\n",
      "\n",
      "### Step-by-Step Analysis:\n",
      "\n",
      "1. **B is next to C**:\n",
      "   - This means B and C must be adjacent.\n",
      "   - Possible pairs for B and C are (B, C) or (C, B). \n",
      "\n",
      "2. **C is not next to D**:\n",
      "   - This means if C is at a certain position, D must not occupy the adjacent positions.\n",
      "\n",
      "3. **A is not next to B**:\n",
      "   - A cannot be in the positions directly adjacent to B.\n",
      "\n",
      "### Possible Arrangements:\n",
      "Let's analyze the arrangements based on the clues.\n",
      "\n",
      "**Case 1: Arrangement with B next to C**\n",
      "\n",
      "#### Pairing B and C as (B, C):\n",
      "- If B is in position 1: \n",
      "  - Options: \n",
      "    - B (1), C (2), A (3), D (4)  -> Valid (A is not adjacent to B)\n",
      "    - B (1), C (2), D (3), A (4)  -> Invalid (D is next to C)\n",
      "  \n",
      "- If B is in position 2: \n",
      "  - Options: \n",
      "    - A (1), B (2), C (3), D (4)  -> Valid (A is not adjacent to B)\n",
      "    - D (1), B (2), C (3), A (4)  -> Valid (A is not adjacent to B)\n",
      "  \n",
      "- If B is in position 3: \n",
      "  - Options: \n",
      "    - A (1), D (2), B (3), C (4)  -> Invalid (A is next to B)\n",
      "    - D (1), A (2), B (3), C (4)  -> Invalid (A is next to B)\n",
      "\n",
      "#### Pairing B and C as (C, B):\n",
      "- If C is in position 1:\n",
      "  - Options:\n",
      "    - C (1), B (2), A (3), D (4) -> Invalid (C is next to D)\n",
      "    \n",
      "- If C is in position 2:\n",
      "  - Options:\n",
      "    - A (1), C (2), B (3), D (4) -> Valid (A is not adjacent to B)\n",
      "    - D (1), C (2), B (3), A (4) -> Valid (A is not adjacent to B)\n",
      "  \n",
      "- If C is in position 3:\n",
      "  - Options:\n",
      "    - A (1), D (2), C (3), B (4) -> Invalid (C is next to D)\n",
      "    - D (1), A (2), C (3), B (4) -> Invalid (C is next to D)\n",
      "\n",
      "### Valid Arrangements:\n",
      "From our analysis, the valid arrangements of A, B, C, D based on all the conditions are:\n",
      "1. A, B, C, D\n",
      "2. D, B, C, A\n",
      "3. A, C, B, D\n",
      "4. D, C, B, A\n",
      "\n",
      "### Final Valid Arrangements:\n",
      "- A, B, C, D\n",
      "- D, B, C, A\n",
      "- A, C, B, D\n",
      "- D, C, B, A\n",
      "\n",
      "These arrangements satisfy all the constraints imposed by the logic puzzle.\n"
     ]
    }
   ],
   "source": [
    "# Simple\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": '''\n",
    "# Intermediate - 1\n",
    "Solve the following logic puzzle step-by-step:\n",
    "Three friends, Alice, Bob, and Carol, have different favorite colors: red, blue, and green. We know that:\n",
    "1. Alice does not like red.\n",
    "2. Bob does not like blue.\n",
    "3. Carol likes green.\n",
    "\n",
    "Determine the favorite color of each friend.\n",
    "\n",
    "Step-by-step solution:\n",
    "1. CarolÏùÄ greenÏùÑ Ï¢ãÏïÑÌïúÎã§. Îî∞ÎùºÏÑú red, blue colorÍ∞Ä ÎÇ®ÏïòÎã§.\n",
    "2. AliceÎäî redÎ•º Ïã´Ïñ¥ÌïòÎØÄÎ°ú blueÎ•º Ï¢ãÏïÑÌïúÎã§.\n",
    "3. BobÏùÄ ÎßàÏßÄÎßâÏúºÎ°ú ÎÇ®ÏùÄ ÏÉâÏù∏ redÎ•º Ï¢ãÏïÑÌïúÎã§.\n",
    "\n",
    "Answer:\n",
    "- Alice: blue\n",
    "- Bob: red\n",
    "- Carol: green\n",
    "            '''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": '''\n",
    "Four people (A, B, C, D) are sitting in a row. We know that:\n",
    "1. A is not next to B.\n",
    "2. B is next to C.\n",
    "3. C is not next to D.\n",
    "\n",
    "Write Possible arrangements\n",
    "            '''\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Î™®Îç∏Ïùò ÏùëÎãµ Ï∂úÎ†•\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c1936-65ba-4d68-aaf3-fe0ac0250c2a",
   "metadata": {},
   "source": [
    "## Prompt Engineering Evaluation ÏûêÎèôÌôî (Using Prompt Bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f62542e-bfa6-481a-afdf-867781cc0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import promptbench as pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dc562df-f780-4125-b1c7-f741df4d851b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All supported datasets: \n",
      "['sst2', 'cola', 'qqp', 'mnli', 'mnli_matched', 'mnli_mismatched', 'qnli', 'wnli', 'rte', 'mrpc', 'mmlu', 'squad_v2', 'un_multi', 'iwslt2017', 'math', 'bool_logic', 'valid_parentheses', 'gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking', 'last_letter_concat', 'numersense', 'qasc', 'bbh', 'drop', 'arc-easy', 'arc-challenge']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a29f35544624855b878e1c5633cfae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db7bc44b1704dd286e7e21eab5b5cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ace736a6d64c1da2c94b16aad27d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6859f8a0fe8b498a93b13ece3fb02550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3865d423898a482dac2864905a6a9d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'content': \"Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
       "  'label': '18'},\n",
       " {'content': 'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?',\n",
       "  'label': '3'},\n",
       " {'content': 'Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?',\n",
       "  'label': '70000'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all supported datasets in promptbench\n",
    "print('All supported datasets: ')\n",
    "print(pb.SUPPORTED_DATASETS)\n",
    "\n",
    "# load a dataset, sst2, for instance.\n",
    "# if the dataset is not available locally, it will be downloaded automatically.\n",
    "dataset_name = \"gsm8k\"\n",
    "dataset = pb.DatasetLoader.load_dataset(dataset_name)\n",
    "\n",
    "# print the first 3 examples\n",
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b60e790a-7cf0-4e92-a8e5-5985fc6511bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All supported models: \n",
      "['google/flan-t5-large', 'llama2-7b', 'llama2-7b-chat', 'llama2-13b', 'llama2-13b-chat', 'llama2-70b', 'llama2-70b-chat', 'phi-1.5', 'phi-2', 'palm', 'gpt-3.5-turbo', 'gpt-4', 'gpt-4-1106-preview', 'gpt-3.5-turbo-1106', 'gpt-4-0125-preview', 'gpt-3.5-turbo-0125', 'gpt-4-turbo', 'gpt-4o', 'vicuna-7b', 'vicuna-13b', 'vicuna-13b-v1.3', 'google/flan-ul2', 'gemini-pro', 'mistralai/Mistral-7B-v0.1', 'mistralai/Mistral-7B-Instruct-v0.1', 'mistralai/Mixtral-8x7B-v0.1', 'mistralai/Mixtral-8x7B-Instruct-v0.1', '01-ai/Yi-6B', '01-ai/Yi-34B', '01-ai/Yi-6B-Chat', '01-ai/Yi-34B-Chat', 'baichuan-inc/Baichuan2-7B-Base', 'baichuan-inc/Baichuan2-13B-Base', 'baichuan-inc/Baichuan2-7B-Chat', 'baichuan-inc/Baichuan2-13B-Chat']\n"
     ]
    }
   ],
   "source": [
    "# print all supported models in promptbench\n",
    "print('All supported models: ')\n",
    "print(pb.SUPPORTED_MODELS)\n",
    "\n",
    "# load a model, gpt-3.5-turbo, for instance.\n",
    "# If model is openai/palm, need to provide openai_key/palm_key\n",
    "# If model is llama, vicuna, need to provide model dir\n",
    "model = pb.LLMModel(model='gpt-3.5-turbo',\n",
    "                    api_key = 'openai_key',\n",
    "                    max_new_tokens=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e760edc5-5124-45c5-b4d8-13b021a57578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All supported methods: \n",
      "['CoT', 'ZSCoT', 'least_to_most', 'generated_knowledge', 'expert_prompting', 'emotion_prompt', 'baseline']\n",
      "Supported datasets for each method: \n",
      "{'CoT': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking'], 'ZSCoT': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking'], 'expert_prompting': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking'], 'emotion_prompt': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking'], 'least_to_most': ['gsm8k', 'last_letter_concat'], 'generated_knowledge': ['csqa', 'numersense', 'qasc'], 'baseline': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking', 'last_letter_concat', 'numersense', 'qasc']}\n"
     ]
    }
   ],
   "source": [
    "# load method\n",
    "# print all methods and their supported datasets\n",
    "print('All supported methods: ')\n",
    "print(pb.SUPPORTED_METHODS)\n",
    "print('Supported datasets for each method: ')\n",
    "print(pb.METHOD_SUPPORT_DATASET)\n",
    "\n",
    "# load a method, emotion_prompt, for instance.\n",
    "# https://github.com/microsoft/promptbench/tree/main/promptbench/prompt_engineering\n",
    "method = pb.PEMethod(method='emotion_prompt',\n",
    "                        dataset=dataset_name,\n",
    "                        verbose=True,  # if True, print the detailed prompt and response\n",
    "                        prompt_id = 1  # for emotion_prompt\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db1cdaab-53b4-48a6-9825-f9ce56e9a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1319 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: openai_key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# if don't set the num_samples, method will use all examples in the dataset\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m results\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/promptbench/prompt_engineering/__init__.py:77\u001b[0m, in \u001b[0;36mPEMethod.test\u001b[0;34m(self, dataset, model, num_samples)\u001b[0m\n\u001b[1;32m     74\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m     76\u001b[0m input_text \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 77\u001b[0m ouput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m res \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m##(.*)\u001b[39m\u001b[38;5;124m'\u001b[39m, ouput)\n\u001b[1;32m     79\u001b[0m pred \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;28;01melse\u001b[39;00m ouput\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/promptbench/prompt_engineering/emotion_prompt.py:44\u001b[0m, in \u001b[0;36mEmotionPrompt.query\u001b[0;34m(self, input_text, model)\u001b[0m\n\u001b[1;32m     40\u001b[0m instr_get_answer \u001b[38;5;241m=\u001b[39m input_text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memotion_prompt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     41\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease output your answer at the end as ##<your answer (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_range\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     42\u001b[0m prompt_get_answer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconvert_text_to_prompt(instr_get_answer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_get_answer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(prompt_get_answer)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/promptbench/models/__init__.py:174\u001b[0m, in \u001b[0;36mLLMModel.__call__\u001b[0;34m(self, input_text, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predicts the output based on the given input text using the loaded model.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/promptbench/models/models.py:424\u001b[0m, in \u001b[0;36mOpenAIModel.predict\u001b[0;34m(self, input_text, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m temperature \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature\n\u001b[1;32m    422\u001b[0m max_new_tokens \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_new_tokens\n\u001b[0;32m--> 424\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    433\u001b[0m     result \u001b[38;5;241m=\u001b[39m [choice\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices]\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/openai/_utils/_utils.py:301\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/openai/_base_client.py:1096\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1084\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1092\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1093\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1094\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1095\u001b[0m     )\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/openai/_base_client.py:856\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    849\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    854\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/openai/_base_client.py:908\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m    906\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 908\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: openai_key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "results = method.test(dataset,\n",
    "                      model,\n",
    "                      num_samples=3 # if don't set the num_samples, method will use all examples in the dataset\n",
    "                      )\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f9748-3dc0-4cfd-bcf5-a4f125e2c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load method\n",
    "# print all methods and their supported datasets\n",
    "print('All supported methods: ')\n",
    "print(pb.SUPPORTED_METHODS)\n",
    "print('Supported datasets for each method: ')\n",
    "print(pb.METHOD_SUPPORT_DATASET)\n",
    "\n",
    "# load a method, emotion_prompt, for instance.\n",
    "# https://github.com/microsoft/promptbench/tree/main/promptbench/prompt_engineering\n",
    "method = pb.PEMethod(method='CoT',\n",
    "                        dataset=dataset_name,\n",
    "                        verbose=True,  # if True, print the detailed prompt and response\n",
    "                        prompt_id = 1  # for emotion_prompt\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e07638-faa9-44d6-8bc6-120ae9296393",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = method.test(dataset,\n",
    "                      model,\n",
    "                      num_samples=3 # if don't set the num_samples, method will use all examples in the dataset\n",
    "                      )\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf01e9-7edf-465d-8d22-20adfed4cedf",
   "metadata": {},
   "source": [
    "## Prompt Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e18a034-dac7-4614-98c2-0321034e4709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf1868b2de14d9791cee6eb3aa2cbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/875 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9dd37cb8d64fc7b801b42f2bf66fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a38ede851f40ddaaee29d753a244bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933be504c7d1418a9c6a9906d426b1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.92M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1174f79ce2d4d5388ddf33837730ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597d563e1f9c48a1bfedba4827b069bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/709M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllmlingua\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptCompressor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# llm_lingua = PromptCompressor(\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     model_name=\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     use_llmlingua2=True, # Whether to use llmlingua-2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m## Use LLMLingua-2-small model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m llm_lingua \u001b[38;5;241m=\u001b[39m \u001b[43mPromptCompressor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_llmlingua2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Whether to use llmlingua-2\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/llmlingua/prompt_compressor.py:89\u001b[0m, in \u001b[0;36mPromptCompressor.__init__\u001b[0;34m(self, model_name, device_map, model_config, open_api_config, use_llmlingua2, llmlingua2_config)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix_bos_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moai_tokenizer \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mencoding_for_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_llmlingua2:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_llmlingua2(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllmlingua2_config)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/llmlingua/prompt_compressor.py:140\u001b[0m, in \u001b[0;36mPromptCompressor.load_model\u001b[0;34m(self, model_name, device_map, model_config)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    135\u001b[0m     device_map\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m device_map \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map:\n\u001b[0;32m--> 140\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMODEL_CLASS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_CLASS\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    152\u001b[0m         model_name,\n\u001b[1;32m    153\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_config,\n\u001b[1;32m    157\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/transformers/modeling_utils.py:3502\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3494\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3495\u001b[0m     (\n\u001b[1;32m   3496\u001b[0m         model,\n\u001b[1;32m   3497\u001b[0m         missing_keys,\n\u001b[1;32m   3498\u001b[0m         unexpected_keys,\n\u001b[1;32m   3499\u001b[0m         mismatched_keys,\n\u001b[1;32m   3500\u001b[0m         offload_index,\n\u001b[1;32m   3501\u001b[0m         error_msgs,\n\u001b[0;32m-> 3502\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3510\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3514\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3521\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/transformers/modeling_utils.py:3926\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3924\u001b[0m                     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, state_dict)\n\u001b[1;32m   3925\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3926\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3930\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3933\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3934\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3935\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3936\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3937\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3938\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3939\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3940\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3941\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3942\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3943\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/transformers/modeling_utils.py:805\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    798\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, model, state_dict_folder, state_dict_index)\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    800\u001b[0m     hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mcheck_quantized_param(model, param, param_name, state_dict))\n\u001b[1;32m    803\u001b[0m ):\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/accelerate/utils/modeling.py:317\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001b[0m\n\u001b[1;32m    315\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 317\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/DL-torch/lib/python3.9/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from llmlingua import PromptCompressor\n",
    "\n",
    "# llm_lingua = PromptCompressor(\n",
    "#     model_name=\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\",\n",
    "#     use_llmlingua2=True, # Whether to use llmlingua-2\n",
    "# )\n",
    "\n",
    "## Use LLMLingua-2-small model\n",
    "llm_lingua = PromptCompressor(\n",
    "    model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n",
    "    use_llmlingua2=True, # Whether to use llmlingua-2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Madam Court, could you please read docket 1239? Certainly. Docket 1239. The Committee on Government Operations, to which was referred on December 1st, 2021, docket number 1239 message an order authorizing the creation of a sheltered market program in conformity with the requirements of general laws. Chapter 30 B Section 18. This authorization applies to contracts for goods, professional services and support services. This authorization is for no more than six contracts, which must be awarded by June 30th, 2022. This sheltered market program shall be available for disadvantaged, minority and women only vendors, for whom there is a demonstrated substantial disparity in the city's 2020 disparities. Study submits a report recommending the order ought to pass. Thank you so much, Madam Clerk. The Chair recognizes Councilor Edwards, chair of the committee. Councilor Edwards. You have the floor. This is this is actually a matter, I believe, sponsored by the. Mayor in Cannes. In conformance with the recommendations from the disparity study and making sure that we opt in to this this pilot program under mass general laws 30 Section 18. Again, it's really just following the recommendations of an already studied issue, which which demonstrates a disparity between minority contractors or women contractors receiving contracts in the city of Boston. So this would allow for us to shepherd and move these six contracts to those already designated groups who have a disadvantage. And I think it's. Really fulfilling a promise. Of making sure that we go through and make sure all aspects of the city government, including the financial benefits, are accessible to people in the city of Boston. I recommend that this pass and I hope that my colleagues will vote for it. Thank you. Thank you so much. Councilor Edward seeks acceptance of the committee report and passage of Docket 1239. Madam Court, could you please call the roll? Certainly. Docket 1239. Councilor Arroyo. Yes. Councilor Arroyo. Yes. Councilor Baker. Councilor Baker. Councilor. Councilor Barker. Council Braden. Councilor Braden. Councilor Campbell. Councilor Campbell. Yes. Councilor Edwards. Yes. Councilor Sabby. George. Councilor Sabby. George. He has Councilor Flaherty. Councilor Flaherty as Councilor Flynn. Councilor Flynn. Yes. Councilor Jane. Yes. Councilor Janey. As Councilor me here. Councilor me here as Councilor Murphy. Councilor Murphy. Yes. And Councilor O'Malley. Yes. Councilor O'Malley. Yes. Madam President, do I get number 1239 has received unanimous vote. Thank you so much. Dockett 1239 has passed and now we will move on to matters recently heard for possible action. Madam Clerk, if you could please read docket 0863. Certainly Docket 0863 order for hearing to discuss pest control and illegal dumping in the city of Boston.\"\"\"\n",
    "compressed_prompt = llm_lingua.compress_prompt(prompt, rate=0.33, force_tokens = ['\\n', '?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e093a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original prompt length: ', len(prompt))\n",
    "print('compressed prompt length: ',len(compressed_prompt['compressed_prompt']))\n",
    "compressed_prompt['compressed_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e80b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
