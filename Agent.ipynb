{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b509f879-45f9-489a-865b-2f8dc53ef952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e14f5a2-e100-4c05-a511-9c94a09311cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(url): \n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(\n",
    "                class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    return docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aee8144-43b2-4ea2-840f-00d23ad9973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever():\n",
    "    urls = [\n",
    "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "        \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "        \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    ]\n",
    "    \n",
    "    docs = []\n",
    "    \n",
    "    for url in urls:\n",
    "        docs.append(load(url))\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "    # retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "    retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 6})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3443e8-7799-49e4-9d1b-08dce97d117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_retriever(question, retriever):\n",
    "    retriver_result = retriever.invoke(question)\n",
    "    return str(retriver_result), \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2915601b-d8f8-4638-bddc-347160457ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_checker(question, docs):\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"docs의 page_content 내용 중에 qeustion과 관련이 있는 정보만 뽑아줘. 리스트 형태로 들어오고, 관련이 있으면 o, 없으면 x로 출력해줘. 판단할 때 굉장히 넓고 자비로운 마음으로 너무 빡빡하지 않게 평가해줘 ex) [o, o, o, o, x, o]\"\n",
    "            }, \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": 'question: ' + question + '\\ndocs:' + docs\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    flag = response.choices[0].message.content\n",
    "    print(flag)\n",
    "    \n",
    "\n",
    "    parser = JsonOutputParser()\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "하나라도 x가 존재한다면 no, 모두 o라면 yes라고 답변해줘. \n",
    "만약 데이터가 없다면 no로 답변해줘.\n",
    "\n",
    "Answer the user query.\\n{format_instructions}\\n{query}\\n\n",
    "key는 related로 해줘\n",
    "    \"\"\",\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    result = chain.invoke({\"query\": flag})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b21b5449-094d-4501-93e8-75e0270d22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevance(question, docs):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"docs의 page_content 내용 중에 qeustion과 관련이 있는 정보만 뽑아줘. 리스트 형태로 들어오고, 관련이 있으면 o, 없으면 x로 출력해줘. ex) [o, o, o, o, x, o]\"\n",
    "            }, \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": 'question: ' + question + '\\ndocs:' + docs\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    flag = response.choices[0].message.content\n",
    "    print(flag)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"flag가 o인 docs만 골라 배열 형태로 답변해줘.\\n 출력 형식: [{source: ~~, page_content: ~}]\\nflag:\\n\" + flag + \"\\ndocs:\\n\" + docs\n",
    "            }, \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": 'question: ' + question + '\\ndocs:' + docs\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    docs = response.choices[0].message.content\n",
    "\n",
    "    return docs, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b54147-9f08-4813-9404-9a3336b88013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, docs):\n",
    "    query = \"question: \" + question + \"\\n\\n\"\n",
    "    query += \"content: \" + docs\n",
    "    \n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"content를 참고하여 question의 답변을 작성해줘\"\n",
    "            }, \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a18b452-9916-4b14-9c07-55f6b1c60699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hallucination_checker(answer, docs):\n",
    "    parser = JsonOutputParser()\n",
    "    \n",
    "    query = \"answer: \" + answer + \"\\n\\n\"\n",
    "    query += \"docs: \" + docs\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "유저의 answer과 docs가 들어오면, docs에 근거하여 답변이 잘 작성되었는지 확인해줘.\n",
    "docs에 없는 내용을 지어냈는지 확인해야해.\n",
    "말을 지어냈다면,  yes\n",
    "docs에 근거했다면, no\n",
    "답변해줘\n",
    "\n",
    "Answer the user query.\\n{format_instructions}\\n{query}\\n\n",
    "key 값은 hallucination으로 해줘\n",
    "    \"\"\",\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    result = chain.invoke({\"query\": query})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ad22662-9d5c-424c-a4f8-b3bbebd7a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final(answer, docs):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"content에서 source를 뽑아서, [answer 내용]\\n source: [source], [제목] 형태로 적어줘\"\n",
    "            }, \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": 'answer: ' + answer + '\\ndocs:' + docs\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1863a453-651a-41b5-a7de-7739923cd0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planning_llm(question):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "사용자의 질문이 들어오면 관련 정보를 검색하고 답변하는 시스템을 만드려고 해.\n",
    "너의 역할은 시스템의 현재 단계 정보가 들어오면 다음 단계의 함수로 전달하는 일이야.\n",
    "입력값이 들어오면 함수의 이름만 반환해줘\n",
    "\n",
    "[제한사항]\n",
    "출력 포멧은 json, key 이름은 \"function\"으로 정확하게 지켜야 합니다.\n",
    "```json ``` 문자는 제외해야 합니다.\n",
    "    \n",
    "[시스템 flow]\n",
    "    0. start: 처음 시작 단계. 유저가 질문을 입력한 직후.\n",
    "    \n",
    "    1. search_retriever: vectorstore에서 사용자 질문과 유사한 정보 검색\n",
    "    \n",
    "    2. relevance_checker: 1번 검색 결과가 사용자 질문과 진짜 유사한지 검증. 만약 유사하지 않다면 2-1번으로 돌아감. 유사하다면 3번으로 넘어감.\n",
    "    출력 예시: {related: yes}\n",
    "\n",
    "    2-1. select_relevance: 1번 검색 결과에서 사용자 질문과 유사한 정보만 뽑음. 이 작업 이후 2번으로 넘어감.\n",
    "    \n",
    "    3. generate_answer: 1번 검색 결과와 사용자 질문을 같이 전달하여, 답변을 만듦.\n",
    "    \n",
    "    4. hallucination_checker: 3번 결과와 docs를 비교하여, hallucination이 발생했는지 확인. 만약 할루시에이션이 발생했다면 다시 3번으로 돌아감. 발생하지 않았다면 5번으로 넘어감.\n",
    "    출력 예시: {hallucination: no}\n",
    "    \n",
    "    5. final: 최종적으로 3번의 결과에 출처명 기입\n",
    "    \n",
    "[입력값 형태]\n",
    "{state: start, result: ''}\n",
    "    \n",
    "[출력]\n",
    "    Answer the user query.\\n{format_instructions}\\n{query}\\n\n",
    "    key 값은 function으로 해줘\n",
    "        \"\"\"\n",
    "            }, \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cad181a8-0ebf-4f9d-8880-20d35454ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(question):\n",
    "    retriever = create_retriever()\n",
    "    state = \"start\"\n",
    "    result = \"\"\n",
    "    re_flag = 0\n",
    "    ha_flag = 0\n",
    "    docs = \"\"\n",
    "    answer = \"\"\n",
    "\n",
    "    while(True):\n",
    "        plan = planning_llm(str({'state': state, 'result': result}))\n",
    "        state = eval(plan)['function']\n",
    "\n",
    "        if state == \"search_retriever\":\n",
    "            print(\"#### search_retriever ####\")\n",
    "            docs, result = search_retriever(question, retriever)\n",
    "            print(result)\n",
    "            \n",
    "        elif state == \"relevance_checker\":\n",
    "            if re_flag > 1:\n",
    "                print(\"#### failed: not relevant ####\")\n",
    "                break;\n",
    "\n",
    "            print(\"#### relevance_checker ####\")\n",
    "            result = relevance_checker(question, docs)\n",
    "            re_flag += 1\n",
    "            print(result)\n",
    "            print()\n",
    "\n",
    "        elif state == \"select_relevance\":\n",
    "            print(\"#### select_relevance ####\")\n",
    "            docs, result = select_relevance(question, docs)\n",
    "            print(result)\n",
    "            \n",
    "        elif state == \"generate_answer\":\n",
    "            print(\"#### generate_answer ####\")\n",
    "            answer, result = generate_answer(question, docs)\n",
    "            print(result)\n",
    "            \n",
    "        elif state == \"hallucination_checker\":\n",
    "            print(ha_flag)\n",
    "            if ha_flag > 1:\n",
    "                print(\"#### failed: hallucination ####\")\n",
    "                break;\n",
    "\n",
    "            print(\"#### hallucination_checker ####\")\n",
    "            result = hallucination_checker(answer, docs)\n",
    "            ha_flag += 1\n",
    "            print(result)\n",
    "            print()\n",
    "            \n",
    "        elif state == \"final\":\n",
    "            print(\"#### final ####\")\n",
    "            result = final(answer, docs)\n",
    "            print(\"#### End! ####\")\n",
    "            print(result)\n",
    "            break;\n",
    "        else:\n",
    "            print(\"#### Error! ####\")\n",
    "            print(plan)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b89c59eb-c0bb-4839-8572-209d4bce44c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### search_retriever ####\n",
      "\n",
      "#### relevance_checker ####\n",
      "[o, o, x, x, x, x]\n",
      "{'related': 'no'}\n",
      "\n",
      "#### select_relevance ####\n",
      "[o, o, x, x, x, o]\n",
      "\n",
      "#### relevance_checker ####\n",
      "[o, o, x]\n",
      "{'related': 'no'}\n",
      "\n",
      "#### select_relevance ####\n",
      "[o, o, x]\n",
      "\n",
      "#### failed: not relevant ####\n"
     ]
    }
   ],
   "source": [
    "main(\"agent memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d722589-b66b-49e3-b185-c108cbca6d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### search_retriever ####\n",
      "\n",
      "#### relevance_checker ####\n",
      "[o, o, o, x, x]\n",
      "{'related': 'no'}\n",
      "\n",
      "#### select_relevance ####\n",
      "[o, o, o, x, x, x]\n",
      "\n",
      "#### relevance_checker ####\n",
      "[o, o, o]\n",
      "{'related': 'yes'}\n",
      "\n",
      "#### generate_answer ####\n",
      "\n",
      "0\n",
      "#### hallucination_checker ####\n",
      "{'hallucination': 'no'}\n",
      "\n",
      "#### final ####\n",
      "#### End! ####\n",
      "answer: Chain-of-Thought (CoT) prompting is a technique that involves generating a series of short sentences to explain reasoning process step by step, which is referred to as reasoning chains or rationales. This approach is especially beneficial for complex reasoning tasks, particularly when using large models (those with more than 50 billion parameters). While it offers significant advantages in complicated scenarios, simple tasks see only slight improvements from employing CoT prompting. There are primarily two types of CoT prompts, one of which is few-shot CoT, where the model is prompted with a few examples containing high-quality reasoning chains, either manually written or generated by the model itself.  \n",
      "source: [https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/], [Chain-of-Thought (CoT)]\n"
     ]
    }
   ],
   "source": [
    "main(\"What is CoT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f9bc6f9-5e25-4284-95d1-df5181c3ecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### search_retriever ####\n",
      "\n",
      "#### relevance_checker ####\n",
      "[o, x, x, x, x]\n",
      "{'related': 'no'}\n",
      "\n",
      "#### select_relevance ####\n",
      "[x, x, x, x, x, x]\n",
      "\n",
      "#### relevance_checker ####\n",
      "[]\n",
      "{'related': 'no'}\n",
      "\n",
      "#### select_relevance ####\n",
      "[]\n",
      "\n",
      "#### failed: not relevant ####\n"
     ]
    }
   ],
   "source": [
    "main(\"I like apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac959c7-887e-432d-a547-297e9321cb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
